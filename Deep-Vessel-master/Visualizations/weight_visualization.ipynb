{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "#import matplotlib as mp\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm\n",
    "#from skimage import img_as_float, img_as_uint\n",
    "from skimage import io\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "mean_img = None\n",
    "\n",
    "# Hyper Params\n",
    "PATCH_DIM = 31\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 5e-4\n",
    "KEEP_PROB = 0.6\n",
    "NUM_CLASSES = 2\n",
    "FCHU1 = 256               # Fully connected layer 1 hidden units\n",
    "MODEL_NAME = '19'\n",
    "MODEL_PATH = '../../Data/models/model19/model.ckpt-5399'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_layer(x,train_phase,scope_bn):\n",
    "    \"\"\"Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n",
    "        \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "        Internal Covariate Shift\"\n",
    "        Sergey Ioffe, Christian Szegedy\n",
    "      Can be used as a normalizer function for conv2d and fully_connected.\n",
    "    \"\"\"\n",
    "    \n",
    "    bn_train = batch_norm(x, decay=0.999, center=True, scale=True, updates_collections=None,\n",
    "    is_training=True, reuse=None, # is this right?\n",
    "    trainable=True, scope=scope_bn)\n",
    "    \n",
    "    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True, updates_collections=None,\n",
    "    is_training=False, reuse=True, # is this right?\n",
    "    trainable=True, scope=scope_bn)\n",
    "    \n",
    "    z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "    \n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed from the downloaded data in the .run() loop, below.\n",
    "    Args:\n",
    "        batch_size: The batch size will be baked into both placeholders.\n",
    "    Returns:\n",
    "        images_placeholder: Images placeholder.\n",
    "        labels_placeholder: Labels placeholder.\n",
    "    \"\"\"\n",
    "    # Note that the shapes of the placeholders match the shapes of the full\n",
    "    # image and label tensors, except the first dimension is now batch_size\n",
    "    # rather than the full size of the train or test data sets.\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, PATCH_DIM**2*3))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size, NUM_CLASSES))\n",
    "    return images_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pd.read_pickle('../../Data/mean_normalised_df_no_class_bias.pkl') \n",
    "#mean_img = pd.read_pickle('../../Data/mean_img_no_class_bias.pkl')\n",
    "\n",
    "    \n",
    "\n",
    "images, labels_placeholder = placeholder_inputs(BATCH_SIZE)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "train_phase = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#---------------------------Inference------------------------------------\n",
    "layers = dict()\n",
    "with tf.variable_scope('h_conv1') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[4, 4, 3, 64], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.05))\n",
    "        \n",
    "        x_image = tf.reshape(images, [-1,PATCH_DIM,PATCH_DIM,3])\n",
    "        x_image_bn = batch_norm_layer(x_image, train_phase, scope.name)\n",
    "        h_conv1_z = tf.nn.conv2d(x_image_bn, weights, strides=[1, 1, 1, 1], padding='VALID') + biases\n",
    "        h_conv1 = tf.nn.relu(h_conv1_z, name=scope.name)\n",
    "        layers['h_conv1'] = h_conv1_z\n",
    "        \n",
    "with tf.variable_scope('h_conv2') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[4, 4, 64, 64], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.05))\n",
    "        h_conv1_bn = batch_norm_layer(h_conv1, train_phase, scope.name)\n",
    "        h_conv2_z = tf.nn.conv2d(h_conv1_bn, weights, strides=[1, 1, 1, 1], padding='SAME')+biases\n",
    "        h_conv2 = tf.nn.relu(h_conv2_z, name=scope.name)\n",
    "        layers['h_conv2'] = h_conv2_z\n",
    "    \n",
    "h_pool1 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name='h_pool1')\n",
    "    \n",
    "with tf.variable_scope('h_conv3') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[4, 4, 64, 64], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        biases = tf.get_variable('biases', shape=[64], initializer=tf.constant_initializer(0.05))\n",
    "        h_pool1_bn = batch_norm_layer(h_pool1, train_phase, scope.name)\n",
    "        h_conv3_z = tf.nn.conv2d(h_pool1_bn, weights, strides=[1, 1, 1, 1], padding='SAME')+biases\n",
    "        h_conv3 = tf.nn.relu(h_conv3_z, name=scope.name)\n",
    "        layers['h_conv3'] = h_conv3_z\n",
    "        \n",
    "h_pool2 = tf.nn.max_pool(h_conv3, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name='h_pool2')    \n",
    "    \n",
    "with tf.variable_scope('h_fc1') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[7**2*64, FCHU1], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.get_variable('biases', shape=[FCHU1], initializer=tf.constant_initializer(0.05))\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        #h_pool2_flat_bn = batch_norm_layer(h_pool2_flat, train_phase, scope.name)\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, weights) + biases, name = 'h_fc1')\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "with tf.variable_scope('h_fc2') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[FCHU1, NUM_CLASSES], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.get_variable('biases', shape=[NUM_CLASSES])\n",
    "        #h_fc1_drop_bn = batch_norm_layer(h_fc1, train_phase, scope.name)\n",
    "        logits = (tf.matmul(h_fc1_drop, weights) + biases)\n",
    "#--------------------------------------------------------------------------------\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Create a session for running Ops on the Graph.\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = 'h_conv1'\n",
    "channel = 5\n",
    "img_noise = np.random.uniform(size=(31,31,3)) + 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "    a = np.reshape(a, [31,31,3])\n",
    "    io.imshow(a)\n",
    "    \n",
    "def visstd(a, s=0.1):\n",
    "    '''Normalize the image range for visualization'''\n",
    "    return ((a-a.mean())/(a.std()+1e-5))*s + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T(layer):\n",
    "    '''Helper for getting layer output tensor'''\n",
    "    return layers[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_naive(t_obj, img0=img_noise, iter_n=2, step=0.1):\n",
    "    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
    "    t_grad = tf.gradients(t_score, images)[0] # behold the power of automatic differentiation!\n",
    "    \n",
    "    img = img0.copy()\n",
    "    img = [img]\n",
    "    img = np.reshape(img, [1,2883])\n",
    "    for i in range(iter_n):\n",
    "        g, score = sess.run([t_grad, t_score], {images:img, keep_prob:1.0, train_phase: False})\n",
    "        print g.max()\n",
    "        print g.min()\n",
    "        print g.std()\n",
    "        print g.mean()\n",
    "        # normalizing the gradient, so the same step size should work \n",
    "        g /= g.std()+1e-5         # for different layers and networks\n",
    "        \n",
    "        img += g*step\n",
    "        print score\n",
    "        #clear_output()\n",
    "        showarray(visstd(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "0.846883\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEaCAYAAADHWDjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHW8VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54cf4a9850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_naive(T(layer)[:,:,:,channel])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
